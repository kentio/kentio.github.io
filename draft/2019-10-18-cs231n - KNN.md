---
layout:     post
title:      cs231n - KNN
subtitle:   Nothing to do with Neural Networks
date:       2019-10-18
author:     kevin
header-img: img/green-bg.jpg
catalog: true
tags:
    - machine learning
	- reading notes
---





这节课开始介绍第一种分类器: 最邻近分类器(Nearest Neighbor Classifier), 这种分类器与神经网络(Convolutional Neural Network)并没有啥关系, 只是一种最简单的将图片分类的分类器.



他的原理就是:

1. 收集训练集的所有样本和标签并且储存
2. 将待分类图片与每一张图片进行比较(pixel-wise),选出距离最小的一张图,那么将给待分类图片分成此类
3. 没啦



真的是非常简单, 就完全比较像素之间的差异, 设想训练集中有和待分类样本数据一样的图, 那么两者之间的距离就为 0 ,肯定是最近的了, 不过这种方法这么粗暴肯定会有短板, 那就是:

1. 每一个训练集都要存储, 占用巨大的存储空间
2. 一个待分类样本要与所有训练集进行比较,耗时很大
3. 准确度不高



下面给出两幅图，左边是 [CIFAR 10](http://www.cs.toronto.edu/~kriz/cifar.html) 数据集的一些样本，右图第一列是测试样本，后面列出了距离测试样本距离最近的十个类别，看到第八个测试样本，是一匹马，但是最邻近算法却预测出他是辆车，肯定错了。。这种情况可能就是因为他们都有很相似的黑色背景~~(滑稽)~~



![CIFAI 10](https://i.loli.net/2019/10/23/XzI9T4ZQ8PLlyc6.png)



下面来介绍一下最邻近算法怎样计算两张图片之间的距离：这里老师说到了 L1 距离和 L2 距离，我上课的时候还记得有汉明距离，马氏距离，欧氏距离等，但是可能不是用于图像分类，因此这里就说下 L1 距离和 L2 距离



L1 距离就是两张图片中对应的每一个像素点的值都相减取绝对值，然后再求和，得出的便为这两张图之间的距离

![l1-distance](https://i.loli.net/2019/10/23/BOGXpsaErRCUgf3.png)

给出一个 L1 距离的例子，方便理解，也就是说，如果图片很相似的话，两张图间的距离会很小，如果差异很大，那么两张图的距离会很大.

![l1-example](https://i.loli.net/2019/10/23/BOGXpsaErRCUgf3.png)



L2 距离是两张图片中对应的每一个像素点的值都相减取平方，相加再开根号，得出的便为这两张图之间的距离

![l2-distance](https://i.loli.net/2019/10/23/BOGXpsaErRCUgf3.png)



那么我们一般使用 L1 距离还是 L2 距离呢？这里给出了，使用 L1 距离的准确率是 38.6% ，使用 L2 距离的准确率为 35.8% ，呵呵哈哈哈或或那我们以后就选 L1 距离吧(误 ，不过这里又说道 L1 和 L2 距离都是在 P 范数中运用最多的（虽然我并不知道 P 范数是啥），这是个超参数，要在实际应用中尝试才知道。



最邻近算法只取了距离最近的一个类，这样很不好，因为可能会受到噪点干扰，因此改进的方案就是 **K邻近算法(KNN)** ，它选取距离样本最近的 K 个类进行投票。e.g. 测试样本为狗，k = 5 ，距离样本最近的五个类分别是 猫， 蛇， 狗，狗，狗，那么这个样本最终的分类就是狗；如果是 猫，猫，猫，蛇，狗，那么最终分类为猫。



下图给出了直观的解释图，最左边一幅为训练集的分布，右边两幅图的背景表示了测试集的预测情况，可以看到，k = 5 时，样本受噪点影响明显变小了，但是出现了白色的区域，这表示分类模糊的点，因为投票中可能有两个类的票数一样，所以分类器不知道该将其规为哪一类。

![knn](https://i.loli.net/2019/10/23/BOGXpsaErRCUgf3.png)



---



实际应用中，如果用到邻近算法，我们都是用 k邻近，但是这个 K 怎么挑选呢，你可能会说直接在验证集中不断地尝试 k 的取值，选出效果最好时候的 k 值，千万不要这么做，**任何时候，验证集都应该被当成宝贵的资源**，千万不能动用验证集，这是在最后一步验证模型的准确度才能用上的。



这个超参数可以通过验证集(Validation sets)调优。